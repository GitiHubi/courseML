{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_colab_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "254.39999389648438px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2MARYUnCBnX"
      },
      "source": [
        "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/hsg_logo.png?raw=1\">\n",
        "\n",
        "###  Lab 06 - \"Deep Learning - Residual Neural Networks\"\n",
        "\n",
        "Machine Learning (BBWL), University of St. Gallen, Spring Term 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qARaVN2aCBnY"
      },
      "source": [
        "In the last lab you learned about how to utilize a **supervised** (deep) machine learning technique, namely **Convolutional Neural Networks (CNNs)**, to classify tiny images of different objects (birds, ships..) contained in the **CIFAR-10** dataset. \n",
        "\n",
        "**CNN**s are a special type of deep neural network that work particularly well on spatial data, as they break down and capture complex patterns which can be used to classify the content of an image.\n",
        "\n",
        "In this lab, we will examine a more recent technique called **Residual Neural Networks (RNNs)**, an extension of **CNNs**. In general, the data contains low / medium / high-level features that are learned by a neural network and used for classification. The analysis and processing of these features can be extended by adding more depth to a network, i.e. increasing the number of layers it contains. However, adding more depth is not as easy as it sounds. **RNNs** are motivated by the will to add depth to deep classifiers and by the difficulty that this entails.\n",
        "\n",
        "In short, a **RNN** allows the training of much deeper neural networks by stacking **Residual Blocks** on top of each other. These blocks allow the activation layers used throughout the network to be fast-forwarded to deeper layers in the network - which ultimately helps the network solve what is known as the *vanishing gradient* problem. Essentially, this problem is one of the challenges of training deep networks. But more on that in the lab.\n",
        "\n",
        "We will again use the functionality of the `PyTorch` library to implement and train a **Residual Neural Network**. As in the **CNN** lab, we will use the **CIFAR-10** dataset. The network will thus be trained on a set of tiny images to learn a model of the images' content. Upon successful training, we will utilize the learned RNN model to classify so far unseen tiny images into distinct categories such as aeroplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. \n",
        "\n",
        "The figure below illustrates a high-level view on the machine learning process we aim to establish in this lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI8MGRvTCBnY"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/pipeline.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTm6boCrCBnY"
      },
      "source": [
        "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq0kb6HyCBnZ"
      },
      "source": [
        "## 1. Lab Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFrqOJFHCBnZ"
      },
      "source": [
        "After today's lab, you should be able to:\n",
        "\n",
        "> 1. Understand the basic concepts, intuitions and major building blocks of **Residual Neural Networks (CNNs)**.\n",
        "> 2. Know how to **implement and to train a RNN** to learn a model of tiny image data.\n",
        "> 3. Understand how to apply such a learned model to **classify images** images based on their content into distinct categories.\n",
        "> 4. Know how to **interpret and visualize** the model's classification results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wurd8ZEaV7P6"
      },
      "source": [
        "Before we start let's watch a motivational video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGquGNFmV78H"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "#NVIDIA: \"Reinventing Retail with AI | I AM AI\"\n",
        "YouTubeVideo('RAzohJygdmc', width=800, height=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdtF-oERCBnZ"
      },
      "source": [
        "## 2. Setup of the Jupyter Notebook Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyjZTGGtCBnZ"
      },
      "source": [
        "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the `PyTorch`, `Numpy`, `Sklearn`, `Matplotlib`, `Seaborn` and a few utility libraries throughout this lab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEmEkcpICBnZ"
      },
      "source": [
        "# import standard python libraries\n",
        "import os, urllib, io\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZW122sXCBnZ"
      },
      "source": [
        "Import Python machine / deep learning libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oET4mu63CBnZ"
      },
      "source": [
        "# import the PyTorch deep learning library\n",
        "import torch, torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ittwEb-HCBnZ"
      },
      "source": [
        "Import the sklearn classification metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKRY16KyCBnZ"
      },
      "source": [
        "# import sklearn classification evaluation library\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XifkBIATCBnZ"
      },
      "source": [
        "Import Python plotting libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDp3LUGCCBna"
      },
      "source": [
        "# import matplotlib, seaborn, and PIL data visualization libary\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWHhPaLaCBna"
      },
      "source": [
        "Enable notebook matplotlib inline plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvLL_Gc9CBna"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdwA4gPS7HM"
      },
      "source": [
        "Import Google's GDrive connector and mount your GDrive directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZorIH6SpS_62"
      },
      "source": [
        "# import the Google Colab GDrive connector\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "# mount GDrive inside the Colab notebook\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37ZxhkMiTDcF"
      },
      "source": [
        "Create a structure of Colab Notebook sub-directories inside of GDrive to store (1) the data as well as (2) the trained neural network models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr2n_gBrTGG3"
      },
      "source": [
        "# create Colab Notebooks directory\r\n",
        "notebook_directory = '/content/drive/MyDrive/Colab Notebooks'\r\n",
        "if not os.path.exists(notebook_directory): os.makedirs(notebook_directory)\r\n",
        "\r\n",
        " # create data sub-directory inside the Colab Notebooks directory\r\n",
        "data_directory = '/content/drive/MyDrive/Colab Notebooks/data'\r\n",
        "if not os.path.exists(data_directory): os.makedirs(data_directory)\r\n",
        "\r\n",
        " # create models sub-directory inside the Colab Notebooks directory\r\n",
        "models_directory = '/content/drive/MyDrive/Colab Notebooks/models'\r\n",
        "if not os.path.exists(models_directory): os.makedirs(models_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX9FcIfICBna"
      },
      "source": [
        "Set a random `seed` value to obtain reproducible results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGV0g08CBna"
      },
      "source": [
        "# init deterministic seed\n",
        "seed_value = 1234\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRXuEt06CBna"
      },
      "source": [
        "Enable GPU computing by setting the `device` flag and init a `CUDA` seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEtzParxCBna"
      },
      "source": [
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
        "\n",
        "# init deterministic GPU seed\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "\n",
        "# log type of device enabled\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZLXfjKlCBna"
      },
      "source": [
        "Let's determine if we have access to a GPU provided by e.g. Google's COLab environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWIjSIttCBna"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGsCQh9iCBna"
      },
      "source": [
        "## 3. Dataset Download and Data Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvnkt7QsCBna"
      },
      "source": [
        "The **CIFAR-10 database** (**C**anadian **I**nstitute **F**or **A**dvanced **R**esearch) is a collection of images that are commonly used to train machine learning and computer vision algorithms. The database is widely used to conduct computer vision research using machine learning and deep learning methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8fN7MQBCBna"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 500px; height: 500px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/cifar10.png?raw=1\">\n",
        "\n",
        "(Source: https://www.kaggle.com/c/cifar-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0hKX9D2CBnb"
      },
      "source": [
        "Further details on the dataset can be obtained via: *Krizhevsky, A., 2009. \"Learning Multiple Layers of Features from Tiny Images\",  \n",
        "( https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf ).\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZAyAzv4CBnb"
      },
      "source": [
        "The CIFAR-10 database contains **60,000 color images** (50,000 training images and 10,000 validation images). The size of each image is 32 by 32 pixels. The collection of images encompasses 10 different classes that represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Let's define the distinct classs for further analytics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pCAh1YACBnb"
      },
      "source": [
        "cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YKqZ_jNCBnb"
      },
      "source": [
        "Thereby the dataset contains 6,000 images for each of the ten classes. The CIFAR-10 is a straightforward dataset that can be used to teach a computer how to recognize objects in images.\n",
        "\n",
        "Let's download, transform and inspect the training images of the dataset. Therefore, we first will define the directory we aim to store the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNf12SKTCBnb"
      },
      "source": [
        "train_path = data_directory + '/train_cifar10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DoDTOk_CBnb"
      },
      "source": [
        "Now, let's download the training data accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKl8DUY1CBnb"
      },
      "source": [
        "# define pytorch transformation into tensor format\n",
        "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# download and transform training images\n",
        "cifar10_train_data = torchvision.datasets.CIFAR10(root=train_path, train=True, transform=transf, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t5WcOmFCBnb"
      },
      "source": [
        "Verify the volume of training images downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcr3BF10CBnb"
      },
      "source": [
        "# get the length of the training data\n",
        "len(cifar10_train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D-5gGAZCBnb"
      },
      "source": [
        "Furthermore, let's investigate a couple of the training images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQk1vXhkCBnb"
      },
      "source": [
        "# set (random) image id\n",
        "image_id = 1800\n",
        "\n",
        "# retrieve image exhibiting the image id\n",
        "cifar10_train_data[image_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpJUgcSUCBnb"
      },
      "source": [
        "Ok, that doesn't seem easily interpretable ;) Let's first seperate the image from its label information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmN2-TEcCBnb"
      },
      "source": [
        "cifar10_train_image, cifar10_train_label = cifar10_train_data[image_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPRHAR0vCBnb"
      },
      "source": [
        "Great, now we are able to visually inspect our sample image: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtSPZGGBCBnb"
      },
      "source": [
        "# define tensor to image transformation\n",
        "trans = torchvision.transforms.ToPILImage()\n",
        "\n",
        "# set image plot title \n",
        "plt.title('Example: {}, Label: \"{}\"'.format(str(image_id), str(cifar10_classes[cifar10_train_label])))\n",
        "\n",
        "# un-normalize cifar 10 image sample\n",
        "cifar10_train_image_plot = cifar10_train_image / 2.0 + 0.5\n",
        "\n",
        "# plot 10 image sample\n",
        "plt.imshow(trans(cifar10_train_image_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9MxnMNCBnc"
      },
      "source": [
        "Fantastic, right? Let's now decide on where we want to store the evaluation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cakcJ5pHCBnc"
      },
      "source": [
        "eval_path = data_directory + '/eval_cifar10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiQ6cYlRCBnc"
      },
      "source": [
        "And download the evaluation data accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmULQ6HACBnc"
      },
      "source": [
        "# define pytorch transformation into tensor format\n",
        "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# download and transform validation images\n",
        "cifar10_eval_data = torchvision.datasets.CIFAR10(root=eval_path, train=False, transform=transf, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A41L8ci8CBnc"
      },
      "source": [
        "Verify the volume of validation images downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpCX0isoCBnc"
      },
      "source": [
        "# get the length of the training data\n",
        "len(cifar10_eval_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7u7WuxjCBnc"
      },
      "source": [
        "## 4. Neural Network Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dyeS_UqbFV0"
      },
      "source": [
        "In this section we, will implement the architecture of the **neural network** we aim to utilize to learn a model that is capable of classifying the 32x32 pixel CIFAR 10 images according to the objects contained in each image. We will first examine what **RNNs** are, how they differ from **CNNs** and how they can be built. However, before we start with the theory, let's briefly revisit the process to be established. The following cartoon provides a birds-eye view:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqZYpDypbSeb"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/pipeline.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRygo9kCTXIT"
      },
      "source": [
        "### 4.1 ResNet Theoretical Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbtPZmVgCBne"
      },
      "source": [
        "Deep neural networks have led to a series of breakthroughs for image classification. In general, deep networks naturally integrate low / mid / high level features and classifiers in an end-to-end multilayer fashion, and the 'levels' of features can be enriched by the depth (or number of stacked layers) of the network. Driven by the significance of depth, the question arises: **Will stacking more layers improve feature learning and therefore increase the classification capability of a model?**\n",
        "\n",
        "In 2015, in their work **'Deep Residual Learning for Image Recognition'** He et al. (https://arxiv.org/abs/1512.03385) proposed an enhanced deep convolutional neural network architecture. The architecture is referred to as **Residual Neural Networks** since it encompasses so called **Residual Layers** or **Residual Blocks**. Using this architecture, He et al. demonstrated that they were able to outperform a variety of image classification benchmark challenges at the time. We have a closer look into the distinct characteristic of the architecture on the following section of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYXMoAYCdkKd"
      },
      "source": [
        "#### 4.1.1 CNN Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wq5_aOWipGc"
      },
      "source": [
        "Before we dive into **RNNs**, let's briefly revisit **Convolutional Neural Networks (CNNs)** as they are the basis for **RNNs**. Here is a birds-eye view of the CNN we built in the previous lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evTPEgmukTPH"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/cnn_pipeline.png?raw=1\">\r\n",
        "\r\n",
        "(Image of the CNN architecture created via http://alexlenail.me/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVVy-N9VkWcl"
      },
      "source": [
        "Our CNN consisted of two **convolutional layers** and three **fully-connected layers**. In general, convolutional layers are specifically designed to learn a set of **high-level features** (\"patterns\") in the processed images, e.g., tiny edges and shapes. The fully-connected layers utilize the learned features to learn **non-linear feature combinations** that allow for highly accurate classification of the image content into the different image classes of the CIFAR-10 dataset, such as, birds, aeroplanes, horses etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlook2f0lMEE"
      },
      "source": [
        "The implementation of these **convolutional layers** involves three operations: (1) **convolution**, (2) **non-linearity**, and (3) **max-pooling**. Those operations are usually executed in sequential order during the forward pass through a convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rp5bMg_li-S"
      },
      "source": [
        "Here is a graphical illustration of convolutional layers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTXVnZ9lpHJ"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/convolutions.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0tgGK8lwGW"
      },
      "source": [
        "To perform a convolution, a **kernel** is used. A kernel is a small matrix of size 5x5 for example, which we horizontally and vertically slide along the image. We then obtain a dot product of the kernel and the pixel values of the kernel's receptive field. The results of these dot products are then summed up and found in the operation's output.\r\n",
        "\r\n",
        "When sliding a given kernel on an image, two elements are particularly important: the **stride** designates the number of pixels a kernel passes when moving, and the **padding** adds a given number of (blank) pixels on the sides of an image to ensure that the output has the same shape as the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pspYsmd3nb25"
      },
      "source": [
        "The last step of a convolutional layer is the application of a **pooling** operation. These are done in **Max-Pooling Layers**, where max-pooling is a type of pooling. Pooling reduces the size of feature maps (images), and gives the ability to ignore positional shifts or translations in the target image by doing so. A truck is a truck, independently of its position. In short, it down-samples the image.\r\n",
        "\r\n",
        "This is done by (again) defining a kernel (usually 2x2 or 3x3), which is slid along the image (like in a convolution). In Max-Pooling, we only keep the highest pixel value within the receptive field of the kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsOTfWYxpeEi"
      },
      "source": [
        "After having performed the convolutional operations, the network turns to **fully connected layers**. Here is an overview:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkTmBoVfpnn4"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/fullyconnected.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HurdY7OVptiq"
      },
      "source": [
        "After the output of the last max-pooling layer has been flattened (to enable connection), we can safely proceed with our fully connected layers. The fully connected layers use activations functions (e.g. **ReLU**) to learn potential non-linear feature combinations. They follow a brute-force concept where all inputs from one layer are connected to every activation unit of the next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUoDAWAXtn5Y"
      },
      "source": [
        "Finally, a **Softmax** function normalizes our output into a probability distribution consisting of K probabilities, where K is the number of distinct classes. These probabilities can be interpreted as the probability that an image belongs to given classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATN1UHyJdoDl"
      },
      "source": [
        "#### 4.1.2 Residual Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7lQk4WU1qSo"
      },
      "source": [
        "The goal of **Residual Neural Networks** is to ease the training of networks that are a lot deeper (where more layers are stacked on top of each other). The training of very deep neural networks is difficult because of several problems. One is of course the computational cost, but there also exists the so-called **\"vanishing gradient problem\"** - or **\"degradation problem\"**. This denotes a weird phenomenon: as network depth increases, accuracy converges and saturates, but then rapidly degrades. This problem is \"*not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error*\" (He et al., 2015, p.1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fRBwva0rne"
      },
      "source": [
        "A **residual block** or **residual layer** is a group of layers for which the same activation is used. This is done by fast-forwarding the activation to a further layer in the neural network. These fast-forwards are also called **skip-connections** or **shortcuts** - taking the activation from one layer and using it deeper. It is this concept that allows the training of very deep (e.g. even more than 100 layers) networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUvZtXLp6tYJ"
      },
      "source": [
        "Here is a more mathematical explanation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHWk446w8Jy2"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/main_vs_resnet_paths.png?raw=1\">\r\n",
        "\r\n",
        "Credit: \"C4W2L03 Resnets\" by Andrew Ng at Deeplearning.ai, https://www.youtube.com/watch?v=ZILIbUvp5lk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujmBonG8O8-"
      },
      "source": [
        "The **\"normal\"** path from layer to layer is represented in the top half of the above image. The input $x^{[l]}$ for a given layer goes through a Linear operator which computes $z^{[l+1]}$. After this, a non-linearity is applied (we use **ReLU** in our example) to get $x^{[l+1]}$. Then, $x^{[l+1]}$ is fed to another Linear layer to get $z^{[l+2]}$, which is again given to a non-linearity to get $x^{[l+2]}$.\r\n",
        "\r\n",
        "The steps of the normal path in more detail:\r\n",
        ">- Start off with $x^{[l]}$\r\n",
        ">- Compute $z^{[l+1]} = W^{[l+1]}* x^{[l]} + b^{[l+1]}$, where $W$ is the weight matrix and $b$ is the bias vector. This operation constitutes the Linearity.\r\n",
        ">- Feed $z^{[l+1]}$ to the activation function $g$ to get $x^{[l+1]}$. $x^{[l+1]} = g(z^{[l+1]})$. This is the non-linearity.\r\n",
        ">- Compute $z^{[l+2]} = W^{[l+2]}*x^{[l+1]}+b^{[l+2]}$, the second Linearity.\r\n",
        ">- Get $x^{[l+2]} = g(z^{[l+2]})$, by again using the ReLU non-linearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8PPh0Sgw1aJ"
      },
      "source": [
        "The *vanishing gradient problem* simply denotes that the application of the non-linearity (ReLU) results in information loss. Information about the original state of the image is modified by ReLU when it arrives to the next layer. After a certain point (a certain depth/number of layers), this problem is such that the model's performance is actually worse. This is why we experience problems by increasing the **CNN**'s depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrpWcByuByJu"
      },
      "source": [
        "In **RNNs**, the idea is that every additional layer should more easily contain the identity function as one of its elements. With the identity function $f(x) = x$, gradients at every layer are computed with the original input taken into account. By using this identity function, information is not lost, which means that adding another layer to the model, theoretically, can only reduce training error or keep it the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LFaC4i8236Q"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/normal_vs_residual_block.png?raw=1\">\r\n",
        "\r\n",
        "Sources: \r\n",
        ">- He, K., Zhang, X., Ren, S., and, Sun, J. (2015). Deep Residual Learning for Image Recognition\r\n",
        ">- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Dive into Deep Learning. Available at https://d2l.ai/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5w-_hbd2_wt"
      },
      "source": [
        "This figure might help illustrate. Let us designate what needs to be learned in a perfect world by $f(x)$, which we will call the desired underlying mapping. This is what will then be fed to the activation function at the top. The schema on the left represents the *normal* path. The steps within the dotted-line box must directly learn the mapping $f(x)$. The steps within the dotted-line box from the schema on the right (the RNN path), however, aim at learning the **residual mapping $f(x) - x$**. \r\n",
        "\r\n",
        "We can do this because, as put in the original paper, \"if one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions\" (He et al., 2015, p.3).\r\n",
        "\r\n",
        " **$x$**, the **residual**, represents the output from the previous layer which we use as input for the current layer.\r\n",
        "\r\n",
        "After this is done, the original  input **$x$** is added to it to obtain the desired underlying mapping $f(x)$. This is shown by the continuous line transporting **$x$** to the top of the block, just before the activation function - this side-path is the so-called *shortcut* (also called *residual connection* or *skip connection*). This way, no extra computational cost occurs and we avoid information loss, thus enabling deeper networks.\r\n",
        "\r\n",
        "The schema on the right represents a residual block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PduThUNW6vEJ"
      },
      "source": [
        "If we go back to our math, the operations that take place in a **Residual Block** are as follows:\r\n",
        "\r\n",
        ">- Start off with $x^{[l]}$\r\n",
        ">- Instead of making it go through the *normal* path, we select it and fast-forward it to the end of the path, where we add it just before the non-linearity (ReLU). This is the **shortcut**.\r\n",
        ">- This means that $x^{[l+2]}$, which was previsouly $x^{[l+2]} = g(z^{[l+2]})$, becomes $x^{[l+2]} = g(z^{[l+2]} {\\color{red} {+ x^{[l]}}})$\r\n",
        ">- The rest stays the same as in the *normal* path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDOQnc81-IEU"
      },
      "source": [
        "Please be sure to notice that $x^{[l]}$ is injected after the linear operation and before the non-linearity (ReLU). This way, the final activation is effectively fast-forwarded to a later stage (i.e., after adding **$x$**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvXR9l9Y_Xza"
      },
      "source": [
        "#### 4.1.3 RNN Architecture Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FR2UntAovh"
      },
      "source": [
        "Now that we have some intuition as to what a **residual block** is, how it works, and why it is useful, let's see how these can be used. And this is actually quite simple. Like many things in deep learning, we just stack them on top of each other. Here is a visualization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYu8PNfZBFSq"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/blocks.png?raw=1\">\r\n",
        "\r\n",
        "Credit: \"C4W2L03 Resnets\" by Andrew Ng at Deeplearning.ai, https://www.youtube.com/watch?v=ZILIbUvp5lk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXv0DNMcBNaP"
      },
      "source": [
        "Or, as illustrated in the original paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw5Yt9FIDZLw"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/resnet_archi.png?raw=1\">\r\n",
        "\r\n",
        "Sources: He, K., Zhang, X., Ren, S., and, Sun, J. (2015). Deep Residual Learning for Image Recognition (page 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EzhsdFODg7p"
      },
      "source": [
        "You might notice that some of the shortcuts are drawn with dotted lines. We have assumed thus far that $x^{[l+2]}$ and $x^{[l]}$ had the same dimension. If this is not the case, as sometimes input/output channels change dimensions throughout the network, the $x^{[l+2]} = g(z^{[l+2]} + x^{[l]})$ operation (where we add **$x$** to the residual mapping) will become problematic.\r\n",
        "\r\n",
        "To counter this, we can either avoid changing dimensions as much as possible, or add an extra matrix $Ws$ to the equation, which would become: \r\n",
        "$$x^{[l+2]} = g(z^{[l+2]} + Ws*x^{[l]})$$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzRKI-1ZLzGY"
      },
      "source": [
        "Say that $z^{[l+2]}$ and therefore $x^{[l+2]}$ is 256-diemnsional, and that $x^{[l]}$ is 128 dimensional. Then if the extra matrix $Ws$ has dimension $\\mathbb{R}^{[256*128]}$, it makes $Ws*x^{[l]}$ 256-dimensional and all is well. This would allow the addition as the matrices would share the same dimensions. That said, what do we actually put in $Ws$? There are two options: it can either be a matrix of parameters we learned, or a fixed matrix that takes $x^{[l]}$ and zero-pads it to make it 256-dimensional. This adjustment is what is represented by dotted lines in the image.\r\n",
        "\r\n",
        "However, this is just a detail so do not worry too much about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCHOVde7iGme"
      },
      "source": [
        "#### 4.1.4 Training Error Progression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHWfEUv1iMZN"
      },
      "source": [
        "As discussed above and prior to the advent of **RNNs**, training error tended to decrease, saturate and then *increase* along with network depth due to the *vanishing gradient* problem (or *degradation* problem). This is illustrated in the figure below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujZQUTdVjDCx"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/deep_cnn_error.png?raw=1\">\r\n",
        "\r\n",
        "Credit: \"C4W2L03 Resnets\" by Andrew Ng at Deeplearning.ai, https://www.youtube.com/watch?v=ZILIbUvp5lk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HloXAigrjMlV"
      },
      "source": [
        "**RNNs** tackle this issue by fighting information loss that occurs during non-linear operations. Learning **residual mappings/functions** with reference to the layer inputs allows the model's accuracy to *at least* not worsen. Thus, the addition of more layers (the deepening of the model) either does not affect training loss or decreases it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pReGbSZhkLsy"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/rnn_error.png?raw=1\">\r\n",
        "\r\n",
        "Credit: \"C4W2L03 Resnets\" by Andrew Ng at Deeplearning.ai, https://www.youtube.com/watch?v=ZILIbUvp5lk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf7EbDMxS8T1"
      },
      "source": [
        "The theory presented here was based on three amazing **sources**, namely:\r\n",
        "\r\n",
        ">- 'Deep Residual Learning for Image Recognition' by He, K., Zhang, X., Ren, S., and, Sun, J., 2015. Available at https://arxiv.org/pdf/1512.03385.pdf \r\n",
        ">- 'Dive into Deep Learning' by Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Available at https://d2l.ai/index.html\r\n",
        ">- Andrew Ng's classes on YouTube: 'C4W2L03 Resnets' and 'C4W2L04 Why ResNets Work' on the Deeplearning.ai channel. Available at https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w and excellent for those who are not big readers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TONCEuvwTcPe"
      },
      "source": [
        "### 4.2 ResNet Architecture Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOEWQtSAi8JO"
      },
      "source": [
        "In this sub-chapter, we will first build the `ResidualBlock` class. We will be able to call it when we then build the `ResNet` class - our neural network. Since our network achitecture will contain residual blocks, we need to define them in advance. We will finish by instantiating the resulting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbmf3HIThMe"
      },
      "source": [
        "#### 4.2.1 The Residual Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gciYsIgSS_0B"
      },
      "source": [
        "As discussed above, a **residual block** simply denotes that the activation of a layer can be fast-forwarded to a deeper layer in the neural network. As you can observe in the image below, the activation from a previous layer is being added to the final activation of a deeper layer in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdLSXVQNTHH-"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 400px\" src=\"https://github.com/GitiHubi/courseML/blob/master/lab_06/residual_block.png?raw=1\">\r\n",
        "\r\n",
        "Sources: \r\n",
        ">- He, K., Zhang, X., Ren, S., and, Sun, J. (2015). Deep Residual Learning for Image Recognition\r\n",
        ">- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Dive into Deep Learning. Available at https://d2l.ai/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIMAzJYITPJs"
      },
      "source": [
        "Let's implement such a **Residual Block**, as shown above, using `PyTorch`library. Our block will consist of the exact same structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF2zdq28S5Kq"
      },
      "source": [
        "# implement ResNet residual block\r\n",
        "class ResidualBlock(nn.Module):\r\n",
        "\r\n",
        "    # define the class constructor\r\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\r\n",
        "\r\n",
        "        # call super class constructor\r\n",
        "        super(ResidualBlock, self).__init__()\r\n",
        "\r\n",
        "        # init first convolutional layer of residual block\r\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) # batch-normalization\r\n",
        "        self.relu1 = nn.ReLU(inplace=True) # non-linearity\r\n",
        "        \r\n",
        "        # init second convolutional layer of residual block\r\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) # batch-normalization\r\n",
        "        self.relu2 = nn.ReLU(inplace=True) # non-linearity\r\n",
        "        \r\n",
        "        # init down-sample flag\r\n",
        "        self.downsample = downsample # for dimension changes\r\n",
        "\r\n",
        "    # define the block forward pass\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        # determine residual\r\n",
        "        residual = x\r\n",
        "        \r\n",
        "        # case: down-sampling needed\r\n",
        "        if self.downsample:\r\n",
        "            \r\n",
        "            # determine down-sampled residual\r\n",
        "            residual = self.downsample(residual)\r\n",
        "\r\n",
        "        # run forward pass through first layer\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu1(out)\r\n",
        "        \r\n",
        "        # run forward pass through second layer\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        # merge output and residual\r\n",
        "        # the skip connection :) \r\n",
        "        out += residual\r\n",
        "        \r\n",
        "        # run second non-linearity\r\n",
        "        out = self.relu2(out)\r\n",
        "\r\n",
        "        # return residual block output\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzUkTViTs87"
      },
      "source": [
        "#### 4.2.2 RNN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvhdeg7Bcygj"
      },
      "source": [
        "Our RNN, which we name 'ResNet' and aim to implement consists of one **convolutional layer** (which includes batch-normalization and ReLU non-linearity), and then three **residual layers** each consisting of 2 **residual blocks**. The residual blocks have the architecture defined above. \r\n",
        " \r\n",
        "Then, a pooling layer precedes a **fully connected layer**. Finally, a we go through a softmax layer to get our classification probabilities for the 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEl28b2jTyOg"
      },
      "source": [
        "Let's implement a `ResNet` network architecture and subsequently have a more in-depth look into its architectural details:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr30FekUT8Mr"
      },
      "source": [
        "# implement the ResNet architecture\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "    # define the class constructor\r\n",
        "    def __init__(self, layers):\r\n",
        "        \r\n",
        "        # call super class constructor\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        \r\n",
        "        #### feature learning layers\r\n",
        "\r\n",
        "        # init initial convolutional layer\r\n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False)\r\n",
        "        self.bn = nn.BatchNorm2d(16) # batch-normalization\r\n",
        "        self.relu = nn.ReLU(inplace=True) # non-linearity\r\n",
        "\r\n",
        "        # init first residual layer \r\n",
        "        self.layer1 = self.make_residual_layer(in_channels=16, out_channels=16, blocks=layers[0], stride=1)\r\n",
        "        \r\n",
        "        # init second residual layer \r\n",
        "        self.layer2 = self.make_residual_layer(in_channels=16, out_channels=32, blocks=layers[1], stride=1)\r\n",
        "        \r\n",
        "        # init third residual layer \r\n",
        "        self.layer3 = self.make_residual_layer(in_channels=32, out_channels=64, blocks=layers[2], stride=1)\r\n",
        "\r\n",
        "        # init average pooling\r\n",
        "        self.avg_pool = nn.AvgPool2d(8)\r\n",
        "\r\n",
        "        #### feature classification layers\r\n",
        "        \r\n",
        "        # define fully connected layer \r\n",
        "        self.fc = nn.Linear(1024, 10)\r\n",
        "\r\n",
        "        # define log-softmax probability conversion\r\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    # create residual layer \r\n",
        "    def make_residual_layer(self, in_channels, out_channels, blocks, stride=1):\r\n",
        "\r\n",
        "        # init down-sample flag\r\n",
        "        downsample = None\r\n",
        "\r\n",
        "        # init array of residual layer elements \r\n",
        "        layers = []\r\n",
        "        \r\n",
        "        # case: down sampling needed\r\n",
        "        if in_channels != out_channels:\r\n",
        "\r\n",
        "            # init down-sampling layer \r\n",
        "            downsample = nn.Sequential(\r\n",
        "                \r\n",
        "                # init down-sampling convolution\r\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\r\n",
        "                \r\n",
        "                # init down-sampling batch normalization \r\n",
        "                nn.BatchNorm2d(out_channels)\r\n",
        "            )\r\n",
        "        \r\n",
        "        # init and append initial residual block\r\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\r\n",
        "\r\n",
        "        # reset input channels\r\n",
        "        self.in_channels = out_channels\r\n",
        "\r\n",
        "        # iterate over remaining residual blocks\r\n",
        "        for i in range(1, blocks):\r\n",
        "\r\n",
        "            # init and append remaining residual blocks\r\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\r\n",
        "\r\n",
        "        # stack layers \r\n",
        "        residual_layer = nn.Sequential(*layers)\r\n",
        "        \r\n",
        "        # return residual layer \r\n",
        "        return residual_layer\r\n",
        "\r\n",
        "    # define the network forward pass\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        # run through inital convolution\r\n",
        "        x = self.conv(x)\r\n",
        "        \r\n",
        "        # run through initial batch-normalization\r\n",
        "        x = self.bn(x)\r\n",
        "        \r\n",
        "        # run trough first residual block\r\n",
        "        x = self.layer1(x)\r\n",
        "        \r\n",
        "        # run through second residual block\r\n",
        "        x = self.layer2(x)\r\n",
        "        \r\n",
        "        # run through third residual block\r\n",
        "        x = self.layer3(x)\r\n",
        "        \r\n",
        "        # conduct average pooling of learned features\r\n",
        "        x = self.avg_pool(x)\r\n",
        "        \r\n",
        "        # reshape the feature map\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        \r\n",
        "        # run final fully-connected layer\r\n",
        "        x = self.fc(x)\r\n",
        "        \r\n",
        "        # run final log-softmax\r\n",
        "        x = self.logsoftmax(x)\r\n",
        "\r\n",
        "        # return forward pass result\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raWN4nhgdLaD"
      },
      "source": [
        "#### 4.2.3 Model Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6iw-nWUA2u"
      },
      "source": [
        "Now that we have implemented our `ResNet` architecture we are ready to instantiate a network model to be trained. In the following we instantiate a `ResNet` where each residual layer consists of two residual blocks: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brf0PNlQUJY9"
      },
      "source": [
        "resnet_model = ResNet(layers=[2, 2, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU2TF8hFUN3N"
      },
      "source": [
        "Let's push the initialized `ResNet` model to the computing `device` that is enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ZSHAygUOai"
      },
      "source": [
        "resnet_model = resnet_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEEFFTnUWBA"
      },
      "source": [
        "Let's double check if our model was deployed to the GPU if available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g6-fN9RUaFj"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0iU3gE5UfVD"
      },
      "source": [
        "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CyinTz0Ui_a"
      },
      "source": [
        "# print the initialized architectures\r\n",
        "print('[LOG] ResNet architecture:\\n\\n{}\\n'.format(resnet_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1pjsmqQUpgP"
      },
      "source": [
        "Looks like intended? Brilliant! Finally, let's have a look into the number of model parameters that we aim to train in the next steps of the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd2pXAryUw0o"
      },
      "source": [
        "# init the number of model parameters\r\n",
        "num_params = 0\r\n",
        "\r\n",
        "# iterate over the distinct parameters\r\n",
        "for param in resnet_model.parameters():\r\n",
        "\r\n",
        "    # collect number of parameters\r\n",
        "    num_params += param.numel()\r\n",
        "    \r\n",
        "# print the number of model paramters\r\n",
        "print('[LOG] Number of to be trained ResNet model parameters: {}.'.format(num_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-nz4f1HU1XV"
      },
      "source": [
        "Ok, our `ResNet` model encompasses a total of 205'338 model parameters to be trained. That's quite an increase in comparison to the prior Convolutional Neural Network we built in the last lab which had 62'006 parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pKVN-Xd7o0"
      },
      "source": [
        "Now that we have implemented the ResNet, we are ready to train the network. However, before starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the classification error of the true class $c^{i}$ of a given CIFAR-10 image $x^{i}$ and its predicted class $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible. \r\n",
        "\r\n",
        "In this lab we use (similarly to the CNN lab) the **'Negative Log-Likelihood (NLL)'** loss. During training the NLL loss will penalize models that result in a high classification error between the predicted class labels $\\hat{c}^{i}$ and their respective true class label $c^{i}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb-kv2XYCBnd"
      },
      "source": [
        "Let's instantiate the **NLL** via the execution of the following PyTorch command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLeknpaECBnd"
      },
      "source": [
        "# define the optimization criterion / loss function\n",
        "nll_loss = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj9lHXw4CBnd"
      },
      "source": [
        "Let's also push the initialized `nll_loss` computation to the computing `device` that is enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PSfp21aCBnd"
      },
      "source": [
        "nll_loss = nll_loss.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAlksm4jXxD9"
      },
      "source": [
        "Similarly to training the `CIFAR10Net`, we will use the **Stochastic Gradient Descent (SGD) optimization** and set the `learning-rate to 0.001`. Each mini-batch step the optimizer will update the model parameters $\\theta$ values according to the degree of classification error (the NLL loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd5vztwMX548"
      },
      "source": [
        "# define learning rate and optimization strategy\r\n",
        "learning_rate = 0.001\r\n",
        "optimizer = optim.SGD(params=resnet_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQeI3IAPUBrY"
      },
      "source": [
        "## 5. Training the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JzWWBSuevMT"
      },
      "source": [
        "In this section, we will train our neural network model (as implemented in the section above) using the transformed images. More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-bLqVCjCBnd"
      },
      "source": [
        "### 5.1 Preparing the Network Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXEvnjKpe2kA"
      },
      "source": [
        "So far, we have pre-processed the dataset, implemented the ResNet and defined the classification error. Let's now start to train a corresponding model for **1 epoch** and a **mini-batch size of 128** CIFAR-10 images per batch. This implies that the whole dataset will be fed to the ResNet times in chunks of 128 images yielding to **391 mini-batches** (50.000 training images / 128 images per mini-batch) per epoch. After the processing of each mini-batch, the parameters of the network will be updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R5i3GSYfCAr"
      },
      "source": [
        "# specify the training parameters\r\n",
        "num_epochs = 1 # number of training epochs\r\n",
        "mini_batch_size = 128 # size of the mini-batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfHLj3anCBnd"
      },
      "source": [
        "Furthermore, let's specifiy and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0IOt9IpCBnd"
      },
      "source": [
        "cifar10_train_dataloader = torch.utils.data.DataLoader(cifar10_train_data, batch_size=mini_batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgInw__dfGQf"
      },
      "source": [
        "We can verify the length of the training `DataLoader`, which should correspond tp **391 mini-batches**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1TqQm9VCFJI"
      },
      "source": [
        "len(cifar10_train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhnjfwTX_mI"
      },
      "source": [
        "### 5.2 Running the Network Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woqs8JOkfc3v"
      },
      "source": [
        "Finally, we start training the model. The training procedure for each mini-batch is performed as follows: \r\n",
        "\r\n",
        ">1. do a forward pass through the ResNet network, \r\n",
        ">2. compute the negative log-likelihood classification error $\\mathcal{L}^{NLL}_{\\theta}(c^{i};\\hat{c}^{i})$, \r\n",
        ">3. do a backward pass through the ResNet network, and \r\n",
        ">4. update the parameters of the network $f_\\theta(\\cdot)$.\r\n",
        "\r\n",
        "To ensure learning while training our ResNet model, we will monitor whether the loss decreases with progressing training. Based on this evaluation, we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\r\n",
        "\r\n",
        "The following elements of the network training code below should be given particular attention:\r\n",
        " \r\n",
        ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\r\n",
        ">- `optimizer.step()` updates the network parameters based on the gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdpLVWv62Dud"
      },
      "source": [
        "Based on the loss magnitude of a certain mini-batch PyTorch automatically computes the gradients. But even better, based on the gradient, the library also helps us in the optimization and update of the network parameters $\\theta$.\n",
        "\n",
        "Each mini-batch step the optimizer will update the model parameters $\\theta$ values according to the degree of classification error (the NLL loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugrq6cG6hHIa"
      },
      "source": [
        "Using the code below, let's run the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l6zWaSOYdCv"
      },
      "source": [
        "# init collection of training epoch losses\r\n",
        "train_epoch_losses = []\r\n",
        "\r\n",
        "# set the model in training mode\r\n",
        "resnet_model.train()\r\n",
        "\r\n",
        "# train the CIFAR10 model\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    \r\n",
        "    # init collection of mini-batch losses\r\n",
        "    train_mini_batch_losses = []\r\n",
        "    \r\n",
        "    # iterate over all-mini batches\r\n",
        "    for i, (images, labels) in enumerate(cifar10_train_dataloader):\r\n",
        "        \r\n",
        "        # push mini-batch data to computation device\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # run forward pass through the network\r\n",
        "        output = resnet_model(images)\r\n",
        "        \r\n",
        "        # reset graph gradients\r\n",
        "        resnet_model.zero_grad()\r\n",
        "        \r\n",
        "        # determine classification loss\r\n",
        "        loss = nll_loss(output, labels)\r\n",
        "        \r\n",
        "        # run backward pass\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        # update network paramaters\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # collect mini-batch reconstruction loss\r\n",
        "        train_mini_batch_losses.append(loss.data.item())\r\n",
        "        \r\n",
        "        # print mini-batch loss\r\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\r\n",
        "        print('[LOG {}] epoch: {}, batch: {} train-loss: {}'.format(str(now), str(epoch+1), str(i+1), str(loss.item())))\r\n",
        "\r\n",
        "    # determine mean min-batch loss of epoch\r\n",
        "    train_epoch_loss = np.mean(train_mini_batch_losses)\r\n",
        "    \r\n",
        "    # print epoch loss\r\n",
        "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\r\n",
        "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch+1), str(train_epoch_loss)))\r\n",
        "\r\n",
        "    # set filename of actual model\r\n",
        "    model_name = 'cifar10_model_epoch_{}.pth'.format(str(epoch))\r\n",
        "\r\n",
        "    # save current model to GDrive models directory\r\n",
        "    torch.save(resnet_model.state_dict(), os.path.join(models_directory, model_name))\r\n",
        "    \r\n",
        "    # determine mean min-batch loss of epoch\r\n",
        "    train_epoch_losses.append(train_epoch_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYvfaxHfYkLV"
      },
      "source": [
        "Upon successfull training let's visualize and inspect the loss per training iteration (mini-batch):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGLQI5veYjYT"
      },
      "source": [
        "# prepare plot\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_subplot(111)\r\n",
        "\r\n",
        "# add grid\r\n",
        "ax.grid(linestyle='dotted')\r\n",
        "\r\n",
        "# plot the training epochs vs. the epochs' classification error\r\n",
        "ax.plot(np.array(range(1, len(train_mini_batch_losses)+1)), train_mini_batch_losses, label='mini-batch loss (blue)')\r\n",
        "\r\n",
        "# add axis legends\r\n",
        "ax.set_xlabel(\"[training mini-batch $mb_i$]\", fontsize=10)\r\n",
        "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\r\n",
        "\r\n",
        "# set plot legend\r\n",
        "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\r\n",
        "\r\n",
        "# add plot title\r\n",
        "plt.title('Training Iterations $mb_i$ vs. Classification Error $L^{NLL}$', fontsize=10);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjedkOnTgOQQ"
      },
      "source": [
        "Ok, fantastic. The training error converges nicely. We could definitely train the network a couple more epochs until the error converges. But let's keep the 1 training epoch for now and continue with evaluating our trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmk-zpedCBnd"
      },
      "source": [
        "## 6. Evaluation of the Trained Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJjq_gHACBng"
      },
      "source": [
        "We will conduct our evaluation based on a model that was already pre-trained for a total of 200 training epochs. Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load a pre-trained snapshot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmA2UFS5CBng"
      },
      "source": [
        "# restore pre-trained model snapshot\n",
        "best_model_name = 'https://raw.githubusercontent.com/GitiHubi/courseML/master/lab_06/models/cifar10_resnet_model_epoch_199.pth'\n",
        "\n",
        "# read stored model from the remote location\n",
        "model_bytes = urllib.request.urlopen(best_model_name)\n",
        "\n",
        "# load model tensor from io.BytesIO object\n",
        "model_buffer = io.BytesIO(model_bytes.read())\n",
        "\n",
        "# init pre-trained model class\n",
        "best_resnet_model = ResNet(layers=[2, 2, 2])\n",
        "\n",
        "# load pre-trained models\n",
        "best_resnet_model.load_state_dict(torch.load(model_buffer, map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AseV_zY2CBng"
      },
      "source": [
        "Let's again inspect if the pre-trained model was loaded successfully: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UtRWTQUCBng",
        "scrolled": true
      },
      "source": [
        "# set model in evaluation mode\n",
        "best_resnet_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdjriZZ2CBne"
      },
      "source": [
        "In order to evaluate our trained model, we need to feed the CIFAR10 images reserved for evaluation (the images that we didn't use as part of the training process) through the model. Therefore, let's again define a corresponding PyTorch data loader that feeds the image tensors to our neural network: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1qrxLfrCBne"
      },
      "source": [
        "cifar10_eval_dataloader = torch.utils.data.DataLoader(cifar10_eval_data, batch_size=8, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU5xY-NuCJOt"
      },
      "source": [
        "len(cifar10_eval_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb8XGNNgCBng"
      },
      "source": [
        "Similarly to the **CNN** we implemented in the last lab, we will now evaluate the pre-trained `ResNet` model based on the mini-batches of the evaluation dataset to derive the mean negative log-likelihood loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMcAeNG_CBng"
      },
      "source": [
        "# init collection of mini-batch losses\n",
        "eval_mini_batch_losses = []\n",
        "\n",
        "# iterate over all-mini batches\n",
        "for i, (images, labels) in enumerate(cifar10_eval_dataloader):\n",
        "\n",
        "    # run forward pass through the network\n",
        "    output = best_resnet_model(images)\n",
        "\n",
        "    # determine classification loss\n",
        "    loss = nll_loss(output, labels)\n",
        "\n",
        "    # collect mini-batch reconstruction loss\n",
        "    eval_mini_batch_losses.append(loss.data.item())\n",
        "\n",
        "# determine mean min-batch loss of epoch\n",
        "eval_loss = np.mean(eval_mini_batch_losses)\n",
        "\n",
        "# print epoch loss\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix7mqb6qCBng"
      },
      "source": [
        "Ok, great. The evaluation loss obtained by `ResNet` model looks significantly lower compared to the one achieved by the `CIFAR10Net` model in the **CNN** lab. Let's now inspect a few sample predictions to get an impression of the model quality. Therefore, we will again pick a random image of our evaluation dataset and retrieve its PyTorch tensor as well as the corresponding label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMR5_fUmCBng"
      },
      "source": [
        "# select a sample image id\n",
        "image_id = 5\n",
        "\n",
        "# obtain the image from the dataloader\n",
        "cifar10_eval_image = iter(cifar10_eval_dataloader).next()[0][image_id]\n",
        "\n",
        "# obtain the true label of the image\n",
        "label = cifar10_eval_data.targets[image_id]\n",
        "\n",
        "# obtain the model's class prediction\n",
        "prediction = torch.argmax(best_resnet_model(cifar10_eval_image.unsqueeze(0)), dim=1).item()\n",
        "\n",
        "# define tensor to image transformation\n",
        "trans = torchvision.transforms.ToPILImage()\n",
        "\n",
        "# set image plot title \n",
        "plt.title('Example: {}, Label: {}, Prediction: {}'.format(str(image_id), str(cifar10_classes[label]), str(cifar10_classes[prediction])))\n",
        "\n",
        "# un-normalize cifar 10 image sample\n",
        "cifar10_eval_image_plot = cifar10_eval_image / 2.0 + 0.5\n",
        "\n",
        "# plot cifar 10 image sample\n",
        "plt.imshow(trans(cifar10_eval_image_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EAFOdYQCBng"
      },
      "source": [
        "Let's now obtain the predictions for all the `CIFAR10` images of the evaluation data using the pre-trained `ResNet`model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhbrFypRQVa"
      },
      "source": [
        "# init collection of mini-batch losses\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# iterate over all-mini batches\r\n",
        "for batch, (images, labels) in enumerate(cifar10_eval_dataloader):\r\n",
        "\r\n",
        "    # run forward pass through the network\r\n",
        "    output = best_resnet_model(images)\r\n",
        "\r\n",
        "    # collect predictons\r\n",
        "    batch_predictions = torch.argmax(output, dim=1)\r\n",
        "\r\n",
        "    # print log each 100 evaluation mini-batch\r\n",
        "    if i % 100 == 0:\r\n",
        "\r\n",
        "      # print mini-batch loss loss\r\n",
        "      now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\r\n",
        "      print('[LOG {}] {} prediction(s) collected'.format(str(now), str(i)))\r\n",
        "\r\n",
        "    # collect mini-batch reconstruction loss\r\n",
        "    predictions.extend(batch_predictions.detach().numpy().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMP62TaJCBng"
      },
      "source": [
        "Let's obtain the overall classification accuracy of the trained `ResNet` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njDRm4V-CBng"
      },
      "source": [
        "metrics.accuracy_score(cifar10_eval_data.targets, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUiYdnJBCBng"
      },
      "source": [
        "In addition, let's inspect the confusion matrix of the model predictions to determine major sources of misclassification:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ1Lf8U7CBng"
      },
      "source": [
        "# determine classification matrix of the predicted and target classes\n",
        "mat = confusion_matrix(cifar10_eval_data.targets, predictions)\n",
        "\n",
        "# plot corresponding confusion matrix\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "plt.title('CIFAR-10 ResNet classification matrix')\n",
        "plt.xlabel('[true label]')\n",
        "plt.ylabel('[predicted label]');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLCTtKdo5He"
      },
      "source": [
        "Ok, we can easily see that our current model confuses images of cats and dogs as well as images of trucks and cars quite often. This is again not surprising since those image categories exhibit a high semantic and therefore visual similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGE2uDyJEvS0"
      },
      "source": [
        "## 7. Exercises:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwqYvj98E-OA"
      },
      "source": [
        "We recommend you try the following exercises as part of the lab:\r\n",
        "\r\n",
        "**1. Train the network a couple more epochs and evaluate its prediction accuracy.**\r\n",
        "\r\n",
        "> Increase the number of training epochs up to 2 or 3 epochs and re-run the network training. Load and evaluate the model exhibiting the lowest training loss. What kind of behavior in terms of prediction accuracy can be observed with increasing the training epochs? What are the problems you encounter when increasing the number of epochs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjee5YGPFQQX"
      },
      "source": [
        "# ***************************************************\r\n",
        "# INSERT YOUR CODE HERE\r\n",
        "# ***************************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrRLw4dLFRSs"
      },
      "source": [
        "**2. Optimizer and learning rate.**\r\n",
        "\r\n",
        "> Try changing the **optimizer** (currently *SGD*) and the **learning rate** (currently 0.001) used in the network training phase. How does this affect the training and the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTnj8LQDGGc3"
      },
      "source": [
        "# ***************************************************\r\n",
        "# INSERT YOUR CODE HERE\r\n",
        "# ***************************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pufykd3cCBng"
      },
      "source": [
        "## 8. Lab Summary:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfKxzaCACBng"
      },
      "source": [
        "In this lab, a step by step introduction into **design, implementation, training and evaluation** of Residual Neural Networks (**RNNs**) to classify tiny images of objects is presented. The code and exercises presented in this lab may serves as a starting point for developing more complex, deeper and more tailored RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gurHO79ACBng"
      },
      "source": [
        "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cTIFNw3CBng"
      },
      "source": [
        "# installing the nbconvert library\n",
        "!pip install nbconvert\n",
        "!pip install jupyter_contrib_nbextensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfK0ylUCBng"
      },
      "source": [
        "Let's now convert the Jupyter notebook into a plain Python script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-ocr84CBng"
      },
      "source": [
        "!jupyter nbconvert --to script ml_lab_05.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n5UYL9iUEUI"
      },
      "source": [
        "## 9. References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPErCbeKUGCb"
      },
      "source": [
        ">- 'Deep Residual Learning for Image Recognition' by He, K., Zhang, X., Ren, S., and, Sun, J., 2015\r\n",
        ">- 'Dive into Deep Learning' by Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Available at https://d2l.ai/index.html\r\n",
        ">- Andrew Ng's classes on YouTube: 'C4W2L03 Resnets' and 'C4W2L04 Why ResNets Work' on the Deeplearning.ai channel. Available at https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w and excellent for those who are not big readers."
      ]
    }
  ]
}